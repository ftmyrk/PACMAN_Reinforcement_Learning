{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"13lwpkusGALGU-1dDBf4y1L71ufx1mjPE","authorship_tag":"ABX9TyM1vH6eEthW2bwb+aLEOfsG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYIscxYDSUHg","executionInfo":{"status":"ok","timestamp":1718155480409,"user_tz":420,"elapsed":17981,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"}},"outputId":"d05c6ba9-d4e4-4811-f88e-0769c90e11ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.1)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n","Collecting ale-py\n","  Downloading ale_py-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.25.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.1)\n","Installing collected packages: ale-py\n","Successfully installed ale-py-0.9.0\n"]}],"source":["!pip install gymnasium\n","!pip install ale-py"]},{"cell_type":"code","source":["import gymnasium as gym\n","import ale_py\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import deque\n","import random\n","import matplotlib.pyplot as plt\n","import os\n","import scipy.io as sio\n","\n","# Hyperparameters\n","GAMMA = 0.99\n","BATCH_SIZE = 32\n","REPLAY_MEMORY_SIZE = 10000\n","LR = 0.0005\n","EPSILON_START = 1.0\n","EPSILON_END = 0.1\n","EPSILON_DECAY = 1000000\n","TARGET_UPDATE_FREQ = 1000\n","\n","class QNetwork(nn.Module):\n","    def __init__(self, input_shape, num_actions):\n","        super(QNetwork, self).__init__()\n","        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","        self.fc1 = nn.Linear(6*7*64, 512)\n","        self.fc2 = nn.Linear(512, num_actions)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.conv1(x))\n","        x = torch.relu(self.conv2(x))\n","        x = torch.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1)\n","        x = torch.relu(self.fc1(x))\n","        return self.fc2(x)\n","\n","class DQNAgent:\n","    def __init__(self, state_shape, num_actions):\n","        self.num_actions = num_actions\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f\"Using device: {self.device}\")  # Print the device being used\n","        self.q_network = QNetwork(state_shape, num_actions).to(self.device)\n","        self.target_network = QNetwork(state_shape, num_actions).to(self.device)\n","        self.target_network.load_state_dict(self.q_network.state_dict())\n","        self.optimizer = optim.Adam(self.q_network.parameters(), lr=LR)\n","        self.memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n","        self.epsilon = EPSILON_START\n","        self.steps_done = 0\n","\n","    def select_action(self, state):\n","        self.steps_done += 1\n","        self.epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * np.exp(-1. * self.steps_done / EPSILON_DECAY)\n","        if random.random() > self.epsilon:\n","            with torch.no_grad():\n","                state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(0)\n","                return self.q_network(state).max(1)[1].item()\n","        else:\n","            return random.randrange(self.num_actions)\n","\n","    def store_transition(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def sample_batch(self):\n","        transitions = random.sample(self.memory, BATCH_SIZE)\n","        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*transitions)\n","\n","        state_batch = np.array(state_batch)\n","        action_batch = np.array(action_batch)\n","        reward_batch = np.array(reward_batch)\n","        next_state_batch = np.array(next_state_batch)\n","        done_batch = np.array(done_batch)\n","\n","        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n","\n","    def update_target_network(self):\n","        self.target_network.load_state_dict(self.q_network.state_dict())\n","\n","    def optimize_model(self):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.sample_batch()\n","\n","        state_batch = torch.tensor(state_batch, device=self.device, dtype=torch.float32)\n","        action_batch = torch.tensor(action_batch, device=self.device, dtype=torch.long).unsqueeze(1)\n","        reward_batch = torch.tensor(reward_batch, device=self.device, dtype=torch.float32)\n","        next_state_batch = torch.tensor(next_state_batch, device=self.device, dtype=torch.float32)\n","        done_batch = torch.tensor(done_batch, device=self.device, dtype=torch.float32)\n","\n","        q_values = self.q_network(state_batch).gather(1, action_batch)\n","        next_q_values = self.target_network(next_state_batch).max(1)[0].detach()\n","        expected_q_values = reward_batch + (GAMMA * next_q_values * (1 - done_batch))\n","\n","        loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","def preprocess_frame(frame, grayscale=True):\n","    if grayscale:\n","        if len(frame.shape) == 3:\n","            frame = np.mean(frame, axis=2).astype(np.uint8)\n","        frame = frame[1:176:2, ::2]\n","        frame = np.expand_dims(frame, axis=0)\n","    frame = frame / 255.0\n","    return frame\n","\n","def train(agent, env, num_episodes, best_reward, episode_rewards):\n","    for episode in range(num_episodes):\n","        state, _ = env.reset()\n","        state = preprocess_frame(state)\n","        total_reward = 0\n","        done = False\n","\n","        while not done:\n","            action = agent.select_action(state)\n","            next_state, reward, done, _, _ = env.step(action)\n","            next_state = preprocess_frame(next_state)\n","            agent.store_transition(state, action, reward, next_state, done)\n","            agent.optimize_model()\n","            state = next_state\n","            total_reward += reward\n","\n","        episode_rewards.append(total_reward)\n","\n","        # Save the model after every episode\n","        torch.save({\n","            'model_state_dict': agent.q_network.state_dict(),\n","            'optimizer_state_dict': agent.optimizer.state_dict(),\n","            'best_reward': best_reward,\n","            'episode_rewards': episode_rewards,\n","            'steps_done': agent.steps_done\n","        }, '/content/drive/MyDrive/PACMAN/last_model.pth')\n","\n","        # Save if it's the best model so far\n","        if total_reward > best_reward:\n","            best_reward = total_reward\n","            torch.save({\n","                'model_state_dict': agent.q_network.state_dict(),\n","                'optimizer_state_dict': agent.optimizer.state_dict(),\n","                'best_reward': best_reward,\n","                'episode_rewards': episode_rewards,\n","                'steps_done': agent.steps_done\n","            }, '/content/drive/MyDrive/PACMAN/best_model.pth')\n","            print(f\"New best model saved with reward: {best_reward}\")\n","\n","        # Save the training data to a .mat file\n","        sio.savemat('/content/drive/MyDrive/PACMAN/training_data.mat', {'best_reward': best_reward, 'episode_rewards': episode_rewards})\n","\n","        if episode % TARGET_UPDATE_FREQ == 0:\n","            agent.update_target_network()\n","\n","        print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Best Reward: {best_reward}\")\n","\n","    return episode_rewards\n","\n","env = gym.make('ALE/MsPacman-v5', frameskip=4)\n","num_actions = env.action_space.n\n","state_shape = (1, 88, 80)\n","\n","agent = DQNAgent(state_shape, num_actions)\n","num_episodes = 1000\n","best_reward = 0\n","episode_rewards = []\n","\n","# Load existing model and training data if available\n","if os.path.exists('/content/drive/MyDrive/PACMAN/last_model.pth'):\n","    checkpoint = torch.load('/content/drive/MyDrive/PACMAN/last_model.pth')\n","    agent.q_network.load_state_dict(checkpoint['model_state_dict'])\n","    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    best_reward = checkpoint['best_reward']\n","    episode_rewards = checkpoint['episode_rewards']\n","    agent.steps_done = checkpoint['steps_done']\n","    print(f\"Latest model loaded with reward: {best_reward}\")\n","\n","if os.path.exists('/content/drive/MyDrive/PACMAN/training_data.mat'):\n","    data = sio.loadmat('/content/drive/MyDrive/PACMAN/training_data.mat')\n","    best_reward = data['best_reward'][0][0]\n","    episode_rewards = data['episode_rewards'][0].tolist()\n","    print(f\"Training data loaded. Best reward: {best_reward}\")\n","\n","# Continue training\n","episode_rewards = train(agent, env, num_episodes, best_reward, episode_rewards)\n","\n","# Plotting\n","plt.plot(episode_rewards)\n","plt.xlabel('Episode')\n","plt.ylabel('Total Reward')\n","plt.show()\n","\n","# Load the best model for evaluation or further training\n","checkpoint = torch.load('/content/drive/MyDrive/PACMAN/best_model.pth')\n"],"metadata":{"id":"KoiBXUD7br64","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2d0e418-bc67-4a05-8c33-0425ea951bad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Latest model loaded with reward: 3440.0\n","Training data loaded. Best reward: 3440.0\n","Episode 1, Total Reward: 270.0, Best Reward: 3440.0\n","Episode 2, Total Reward: 230.0, Best Reward: 3440.0\n","Episode 3, Total Reward: 360.0, Best Reward: 3440.0\n","Episode 4, Total Reward: 350.0, Best Reward: 3440.0\n","Episode 5, Total Reward: 450.0, Best Reward: 3440.0\n","Episode 6, Total Reward: 300.0, Best Reward: 3440.0\n","Episode 7, Total Reward: 600.0, Best Reward: 3440.0\n","Episode 8, Total Reward: 250.0, Best Reward: 3440.0\n","Episode 9, Total Reward: 360.0, Best Reward: 3440.0\n","Episode 10, Total Reward: 300.0, Best Reward: 3440.0\n","Episode 11, Total Reward: 320.0, Best Reward: 3440.0\n","Episode 12, Total Reward: 500.0, Best Reward: 3440.0\n","Episode 13, Total Reward: 300.0, Best Reward: 3440.0\n","Episode 14, Total Reward: 250.0, Best Reward: 3440.0\n"]}]},{"cell_type":"code","source":["a = agent.q_network.load_state_dict(torch.load('best_model.pth'))"],"metadata":{"id":"4qUhgh2WVy2e","executionInfo":{"status":"ok","timestamp":1718155848032,"user_tz":420,"elapsed":160,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["b = torch.load('best_model.pth')"],"metadata":{"id":"5j0xZJNiWAoi","executionInfo":{"status":"ok","timestamp":1718157040458,"user_tz":420,"elapsed":200,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["b.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zGSP4yddI-BS","executionInfo":{"status":"ok","timestamp":1718157051544,"user_tz":420,"elapsed":185,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"}},"outputId":"dc070f30-e1e1-4584-ae7c-ab682c9fa2a4"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["data = sio.loadmat('/content/training_data.mat')"],"metadata":{"id":"XX5Kv3MYWhnL","executionInfo":{"status":"ok","timestamp":1718159985255,"user_tz":420,"elapsed":155,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["data['episode_rewards'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fEl7A9g8Enps","executionInfo":{"status":"ok","timestamp":1718159988513,"user_tz":420,"elapsed":160,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"}},"outputId":"fef9a152-ffb3-4914-e06e-4ba052a6e766"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 1031)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# Load the best model for evaluation or further training\n","agent.q_network.load_state_dict(torch.load('best_model.pth'))\n","\n","# Play using the best model and save the gameplay as a video\n","play_and_save_video(agent, env)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"A4e4wBT9UQqB","executionInfo":{"status":"error","timestamp":1717472309609,"user_tz":420,"elapsed":416,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"}},"outputId":"a2ed3c21-48de-4b38-e22d-333234f984c4"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Given groups=1, weight of size [32, 1, 8, 8], expected input[1, 3, 210, 160] to have 1 channels, but got 3 channels instead","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-270a09d08537>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Play using the best model and save the gameplay as a video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplay_and_save_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-40-727e9ee6c8bd>\u001b[0m in \u001b[0;36mplay_and_save_video\u001b[0;34m(agent, env, video_filename)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-727e9ee6c8bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 8, 8], expected input[1, 3, 210, 160] to have 1 channels, but got 3 channels instead"]}]},{"cell_type":"code","source":["# Load the best model for evaluation or further training\n","agent.q_network.load_state_dict(torch.load('best_model.pth'))\n","\n","# Play using the best model and save the gameplay as a video\n","play_and_save_video(agent, env)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"IgmFDCxNOPay","executionInfo":{"status":"error","timestamp":1717472158582,"user_tz":420,"elapsed":175,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"}},"outputId":"a8a17b21-9a64-4831-f18d-efac742a81db"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'tuple' object has no attribute 'shape'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-270a09d08537>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Play using the best model and save the gameplay as a video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplay_and_save_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-34-fa3627f52688>\u001b[0m in \u001b[0;36mplay_and_save_video\u001b[0;34m(agent, env, video_filename)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Get frame dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mframe_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mframe_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"]}]}]}