{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17981,"status":"ok","timestamp":1718155480409,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"},"user_tz":420},"id":"CYIscxYDSUHg","outputId":"d05c6ba9-d4e4-4811-f88e-0769c90e11ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.1)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n","Collecting ale-py\n","  Downloading ale_py-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.25.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.1)\n","Installing collected packages: ale-py\n","Successfully installed ale-py-0.9.0\n"]}],"source":["!pip install gymnasium\n","!pip install ale-py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoiBXUD7br64","outputId":"c2d0e418-bc67-4a05-8c33-0425ea951bad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Latest model loaded with reward: 3440.0\n","Training data loaded. Best reward: 3440.0\n","Episode 1, Total Reward: 270.0, Best Reward: 3440.0\n","Episode 2, Total Reward: 230.0, Best Reward: 3440.0\n","Episode 3, Total Reward: 360.0, Best Reward: 3440.0\n","Episode 4, Total Reward: 350.0, Best Reward: 3440.0\n","Episode 5, Total Reward: 450.0, Best Reward: 3440.0\n","Episode 6, Total Reward: 300.0, Best Reward: 3440.0\n","Episode 7, Total Reward: 600.0, Best Reward: 3440.0\n","Episode 8, Total Reward: 250.0, Best Reward: 3440.0\n","Episode 9, Total Reward: 360.0, Best Reward: 3440.0\n","Episode 10, Total Reward: 300.0, Best Reward: 3440.0\n","Episode 11, Total Reward: 320.0, Best Reward: 3440.0\n","Episode 12, Total Reward: 500.0, Best Reward: 3440.0\n","Episode 13, Total Reward: 300.0, Best Reward: 3440.0\n","Episode 14, Total Reward: 250.0, Best Reward: 3440.0\n"]}],"source":["import gymnasium as gym\n","import ale_py\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import deque\n","import random\n","import matplotlib.pyplot as plt\n","import os\n","import scipy.io as sio\n","\n","# Hyperparameters\n","GAMMA = 0.99\n","BATCH_SIZE = 32\n","REPLAY_MEMORY_SIZE = 10000\n","LR = 0.0005\n","EPSILON_START = 1.0\n","EPSILON_END = 0.1\n","EPSILON_DECAY = 1000000\n","TARGET_UPDATE_FREQ = 1000\n","\n","class QNetwork(nn.Module):\n","    def __init__(self, input_shape, num_actions):\n","        super(QNetwork, self).__init__()\n","        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","        self.fc1 = nn.Linear(6*7*64, 512)\n","        self.fc2 = nn.Linear(512, num_actions)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.conv1(x))\n","        x = torch.relu(self.conv2(x))\n","        x = torch.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1)\n","        x = torch.relu(self.fc1(x))\n","        return self.fc2(x)\n","\n","class DQNAgent:\n","    def __init__(self, state_shape, num_actions):\n","        self.num_actions = num_actions\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f\"Using device: {self.device}\")  # Print the device being used\n","        self.q_network = QNetwork(state_shape, num_actions).to(self.device)\n","        self.target_network = QNetwork(state_shape, num_actions).to(self.device)\n","        self.target_network.load_state_dict(self.q_network.state_dict())\n","        self.optimizer = optim.Adam(self.q_network.parameters(), lr=LR)\n","        self.memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n","        self.epsilon = EPSILON_START\n","        self.steps_done = 0\n","\n","    def select_action(self, state):\n","        self.steps_done += 1\n","        self.epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * np.exp(-1. * self.steps_done / EPSILON_DECAY)\n","        if random.random() > self.epsilon:\n","            with torch.no_grad():\n","                state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(0)\n","                return self.q_network(state).max(1)[1].item()\n","        else:\n","            return random.randrange(self.num_actions)\n","\n","    def store_transition(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def sample_batch(self):\n","        transitions = random.sample(self.memory, BATCH_SIZE)\n","        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*transitions)\n","\n","        state_batch = np.array(state_batch)\n","        action_batch = np.array(action_batch)\n","        reward_batch = np.array(reward_batch)\n","        next_state_batch = np.array(next_state_batch)\n","        done_batch = np.array(done_batch)\n","\n","        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n","\n","    def update_target_network(self):\n","        self.target_network.load_state_dict(self.q_network.state_dict())\n","\n","    def optimize_model(self):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.sample_batch()\n","\n","        state_batch = torch.tensor(state_batch, device=self.device, dtype=torch.float32)\n","        action_batch = torch.tensor(action_batch, device=self.device, dtype=torch.long).unsqueeze(1)\n","        reward_batch = torch.tensor(reward_batch, device=self.device, dtype=torch.float32)\n","        next_state_batch = torch.tensor(next_state_batch, device=self.device, dtype=torch.float32)\n","        done_batch = torch.tensor(done_batch, device=self.device, dtype=torch.float32)\n","\n","        q_values = self.q_network(state_batch).gather(1, action_batch)\n","        next_q_values = self.target_network(next_state_batch).max(1)[0].detach()\n","        expected_q_values = reward_batch + (GAMMA * next_q_values * (1 - done_batch))\n","\n","        loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","def preprocess_frame(frame, grayscale=True):\n","    if grayscale:\n","        if len(frame.shape) == 3:\n","            frame = np.mean(frame, axis=2).astype(np.uint8)\n","        frame = frame[1:176:2, ::2]\n","        frame = np.expand_dims(frame, axis=0)\n","    frame = frame / 255.0\n","    return frame\n","\n","def train(agent, env, num_episodes, best_reward, episode_rewards):\n","    for episode in range(num_episodes):\n","        state, _ = env.reset()\n","        state = preprocess_frame(state)\n","        total_reward = 0\n","        done = False\n","\n","        while not done:\n","            action = agent.select_action(state)\n","            next_state, reward, done, _, _ = env.step(action)\n","            next_state = preprocess_frame(next_state)\n","            agent.store_transition(state, action, reward, next_state, done)\n","            agent.optimize_model()\n","            state = next_state\n","            total_reward += reward\n","\n","        episode_rewards.append(total_reward)\n","\n","        # Save the model after every episode\n","        torch.save({\n","            'model_state_dict': agent.q_network.state_dict(),\n","            'optimizer_state_dict': agent.optimizer.state_dict(),\n","            'best_reward': best_reward,\n","            'episode_rewards': episode_rewards,\n","            'steps_done': agent.steps_done\n","        }, '/content/drive/MyDrive/PACMAN/last_model.pth')\n","\n","        # Save if it's the best model so far\n","        if total_reward > best_reward:\n","            best_reward = total_reward\n","            torch.save({\n","                'model_state_dict': agent.q_network.state_dict(),\n","                'optimizer_state_dict': agent.optimizer.state_dict(),\n","                'best_reward': best_reward,\n","                'episode_rewards': episode_rewards,\n","                'steps_done': agent.steps_done\n","            }, '/content/drive/MyDrive/PACMAN/best_model.pth')\n","            print(f\"New best model saved with reward: {best_reward}\")\n","\n","        # Save the training data to a .mat file\n","        sio.savemat('/content/drive/MyDrive/PACMAN/training_data.mat', {'best_reward': best_reward, 'episode_rewards': episode_rewards})\n","\n","        if episode % TARGET_UPDATE_FREQ == 0:\n","            agent.update_target_network()\n","\n","        print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Best Reward: {best_reward}\")\n","\n","    return episode_rewards\n","\n","env = gym.make('ALE/MsPacman-v5', frameskip=4)\n","num_actions = env.action_space.n\n","state_shape = (1, 88, 80)\n","\n","agent = DQNAgent(state_shape, num_actions)\n","num_episodes = 1000\n","best_reward = 0\n","episode_rewards = []\n","\n","# Load existing model and training data if available\n","if os.path.exists('/content/drive/MyDrive/PACMAN/last_model.pth'):\n","    checkpoint = torch.load('/content/drive/MyDrive/PACMAN/last_model.pth')\n","    agent.q_network.load_state_dict(checkpoint['model_state_dict'])\n","    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    best_reward = checkpoint['best_reward']\n","    episode_rewards = checkpoint['episode_rewards']\n","    agent.steps_done = checkpoint['steps_done']\n","    print(f\"Latest model loaded with reward: {best_reward}\")\n","\n","if os.path.exists('/content/drive/MyDrive/PACMAN/training_data.mat'):\n","    data = sio.loadmat('/content/drive/MyDrive/PACMAN/training_data.mat')\n","    best_reward = data['best_reward'][0][0]\n","    episode_rewards = data['episode_rewards'][0].tolist()\n","    print(f\"Training data loaded. Best reward: {best_reward}\")\n","\n","# Continue training\n","episode_rewards = train(agent, env, num_episodes, best_reward, episode_rewards)\n","\n","# Plotting\n","plt.plot(episode_rewards)\n","plt.xlabel('Episode')\n","plt.ylabel('Total Reward')\n","plt.show()\n","\n","# Load the best model for evaluation or further training\n","checkpoint = torch.load('/content/drive/MyDrive/PACMAN/best_model.pth')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":160,"status":"ok","timestamp":1718155848032,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"},"user_tz":420},"id":"4qUhgh2WVy2e"},"outputs":[],"source":["a = agent.q_network.load_state_dict(torch.load('best_model.pth'))"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":200,"status":"ok","timestamp":1718157040458,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"},"user_tz":420},"id":"5j0xZJNiWAoi"},"outputs":[],"source":["b = torch.load('best_model.pth')"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1718157051544,"user":{"displayName":"Faruk Tamyurek","userId":"01151817370602569000"},"user_tz":420},"id":"zGSP4yddI-BS","outputId":"dc070f30-e1e1-4584-ae7c-ab682c9fa2a4"},"outputs":[{"data":{"text/plain":["odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["b.keys()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM1vH6eEthW2bwb+aLEOfsG","gpuType":"T4","mount_file_id":"13lwpkusGALGU-1dDBf4y1L71ufx1mjPE","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
